"""Collecting Jobs from Google Job Search"""

__author__ = "https://github.com/refnhaldykristian"
__version__ = "2.1.0"

import os
import re
from time import sleep
from datetime import date, datetime, timedelta

import pandas as pd
import sqlalchemy as db
from sqlalchemy.sql import text
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support import expected_conditions as EC

# Environment Variables
DB_USER = os.environ.get("DB_USER")
DB_PASS = os.environ.get("DB_PASS")
DB_HOST = os.environ.get("DB_HOST")
DB_PORT = os.environ.get("DB_PORT")

# Global Variables
today_id = int(datetime.today().strftime("%Y%m%d"))
engine = db.create_engine(f"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/postgres")

def check_etl():
    # Check if the ETL process already executed for today
    df = pd.read_sql("SELECT MAX(id) as id FROM public.check_etl", engine)
    max_id = df["id"].values[0]

    if max_id == today_id:
        print("ETL process already executed for today")
        is_executed = True
    else:
        is_executed = False
    
    return is_executed
    
def get_state():
    # Get Indonesia geographical data from Wikipedia
    url = "https://id.wikipedia.org/wiki/Provinsi_di_Indonesia"

    try:
        df_city = pd.read_html(url)[2]
    except Exception as e:
        print(f"Error retrieving data from {url}: {e}")
        exit()
        
    states = df_city["Provinsi"].values.tolist()
    states = [state[0] for state in states]

    return states

def setup_selenium():
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"

    # Configure selenium
    options = webdriver.ChromeOptions()
    options.add_argument(f"user-agent={user_agent}")
    options.add_argument("--no-sandbox")
    options.add_argument("--headless")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    return driver

def load_page(driver):
    # Scroll to the bottom of the list until all jobs are loaded
    all_jobs_loaded = False
    while not all_jobs_loaded:

        # Get old scroll height
        old_height = driver.execute_script("return document.body.scrollHeight")

        # Scroll to bottom of the list (fetches more jobs)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        # Grant some time for listing to load
        sleep(1)

        # Get current scroll height
        new_height = driver.execute_script("return document.body.scrollHeight")

        # Check if all list were loaded by comparing the scroll heights
        if new_height == old_height:
            # Scroll back to top of the list to start collecting jobs
            driver.execute_script("window.scrollTo(0, 0);")
            all_jobs_loaded = True

def get_listing(driver: webdriver.Chrome):
    list_data = []
    today = datetime.today()

    # Check wheter class v3jTId is exist in the dom
    try:
        driver.find_element(By.CLASS_NAME, "v3jTId")
        print("No job listing found")
        return pd.DataFrame()
    except:
        pass
        
    # Looping through jobs list
    for job in driver.find_elements(By.CLASS_NAME, "EimVGf"):
        # Get job details
        primary: list[str] = job.find_element(By.CLASS_NAME, "u9g6vf").text.split("\n")
        secondary: list[str] = job.find_element(By.CLASS_NAME, "ApHyTb").text.split("\n")
        job_title: str = primary[0]
        company_name: str = primary[1]
        index_2: list[str] = primary[2].split("â€¢")
        via: str = index_2[-1].replace("melalui", "")
        contract: str | None = secondary[-1] if secondary[-1] in ["Kontraktor", "Pekerjaan tetap", "Paruh waktu", "Magang"] else None

        # Initialize variables
        location: str = ""
        city: str = ""
        mapped_city: str = ""
        salary_range: str = ""
        posted_at: str = ""
        date_posted: date | None = None
        min_monthly_salary: float = 0.00
        max_monthly_salary: float = 0.00
        min_daily_salary: float = 0.00
        max_daily_salary: float = 0.00
        min_hourly_salary: float = 0.00
        max_hourly_salary: float = 0.00

        # Uncertain index_2 data
        if len(index_2) > 1:
            location = index_2[0]

        # Uncertain location data
        if len(location) > 1:
            location_list = location.split(",")
            if len(location_list) > 1:
                city = location_list[-2]

        # Uncertain secondary data
        if len(secondary) > 2:
            posted_at = secondary[0]
            salary_range = secondary[1]
        elif len(secondary) > 1:
            if "jt" in secondary[0]:
                salary_range = secondary[0]
            elif "hari" in secondary[0] or "jam" in secondary[0]:
                posted_at = secondary[0]

        # Handle posted_at
        if "hari yang lalu" in posted_at:
            offset = int(posted_at[0])
            date_posted = (today - timedelta(days=offset)).date()
        elif "jam yang lalu" in posted_at:
            offset = int(posted_at[0])
            date_posted = (today - timedelta(hours=offset)).date()

        # Handle salary_range
        if "jt per bulan" in salary_range:
            salary_range = salary_range.replace(",", ".")
            min_monthly_salary = float(re.findall(r"\d+(?:.\d+)?", salary_range)[0]) * 1_000_000
            max_monthly_salary = float(re.findall(r"\d+(?:.\d+)?", salary_range)[-1]) * 1_000_000
            min_daily_salary = min_monthly_salary / 22
            max_daily_salary = max_monthly_salary / 22
            min_hourly_salary = min_daily_salary / 8
            max_hourly_salary = max_daily_salary / 8
        elif "jt per hari" in salary_range:
            salary_range = salary_range.replace(",", ".")
            min_daily_salary = float(re.findall(r"\d+(?:.\d+)?", salary_range)[0]) * 1_000_000
            max_daily_salary = float(re.findall(r"\d+(?:.\d+)?", salary_range)[-1]) * 1_000_000
            min_monthly_salary = min_daily_salary * 22
            max_monthly_salary = max_daily_salary * 22
            min_hourly_salary = min_daily_salary / 8
            max_hourly_salary = max_daily_salary / 8

        # Handle location
        if "Kota " in city or "Kabupaten " in city:
            mapped_city = city.replace("Kota ", "").replace("Kabupaten ", "")
        else:
            mapped_city = city

        data = {
            "index_2": index_2,
            "date_collected": today.date(),
            "date_posted": date_posted,
            "search_category": "",
            "job_title": job_title.strip(),
            "company_name": company_name.strip(),
            "location": location.strip(),
            "city": city.strip(),
            "mapped_city": mapped_city.strip(),
            "via": via.strip(),
            "contract": contract,
            "salary_range": salary_range.strip(),
            "min_monthly_salary": min_monthly_salary,
            "max_monthly_salary": max_monthly_salary,
            "min_daily_salary": min_daily_salary,
            "max_daily_salary": max_daily_salary,
            "min_hourly_salary": min_hourly_salary,
            "max_hourly_salary": max_hourly_salary
        }
        
        # Append data to list
        list_data.append(data)
    
    df_listing = pd.DataFrame(list_data)

    return df_listing

def update_data(df_final: pd.DataFrame):
    # Get previous data from database
    df_old = pd.read_sql("SELECT job_title, company_name, mapped_city, via FROM public.job_listing", engine)

    # Remove rows from df_final that are already in df_old
    df_final = pd.merge(df_final, df_old, on=["job_title", "company_name", "mapped_city", "via"], how="left", indicator=True)
    df_final = df_final[df_final["_merge"] == "left_only"].drop(columns=["_merge"])

    # Save the data to the database
    df_final.drop_duplicates(subset=["job_title", "company_name", "mapped_city", "via"], inplace=True)
    df_final.to_sql("job_listing", engine, if_exists="append", index=False)
    
    print(f"{len(df_final)} data has been succesfully added to database!")

def update_etl():
    # Update the check_etl table
    with engine.connect() as conn:
        statement = text(f"INSERT INTO public.check_etl (id) VALUES ({today_id})")
        conn.execute(statement)
        conn.commit()

# Main Script
def main():
    # Check ETL status
    is_executed = check_etl()

    if is_executed:
        return

    searchs = ["Data+Analyst", "Data+Engineer", "Data+Scientist"]
    driver = setup_selenium()
    df_final = pd.DataFrame()

    for search in searchs:
        driver.get(f"https://www.google.com/search?q={search}+Indonesia&ibp=htl;jobs#htivrt=jobs&fpstate=tldetail&htilrad=-1.0&htidocid")
        search = search.replace("+", " ")

        try:
            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "EimVGf")))
            load_page(driver)
            df_listing = get_listing(driver)
            df_listing["search_category"] = search
            df_final = pd.concat([df_final, df_listing], ignore_index=True)
            print(f"Succesfully get {len(df_final)} {search} jobs")
        except TimeoutException:
            print(f"Request time out while waiting {search} to load")

    # Close the browser
    driver.quit()
    
    # Update the data
    update_data(df_final)
    update_etl()